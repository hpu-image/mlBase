# 集成学习

##   目录

1. 概述
2. 算法概念
3. 算法原理
4. 算法示例
5. 算法代码

## 概述

集成学习通过构建并结合多个学习器来完成学习任务。Boosting 是一族可将弱学习器提升为强学习器的算法。这族算法的工作机制类似:先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注，然后基于调整后的样本分布来训练下一个基学习器;如此重复进行，直至基学习器数目达到事先指定的值，最终将这个基学习器进行加权结合。

Boosting 族算法最著名的代表是 AdaBoost算法

![image-20201118201133519](集成学习.assets\image-20201118201133519.png)

在这个二分类任务中，假定三个分类器在三个测试样本上的表现如上图所示，集成学习的结果通过投票法产生，即“少数服从多数”可得准确度较高，差异度也较高，可以较好地提升集成性能。从而总结出个体学习器要有一定的“准确性”，即学习器不能太坏，并且要有“多样性”，即学习器间要有差异。

## 算法概念

在集成模型中，某个体学习器都属于同一类别，例如都是决策树或者都是神经网络，则称该集成为同质的；某个体学习器包含多种类型的学习算法，例如既有决策树又有神经网络，则称该集成为异质的。

同质集成：个体学习器称为“基学习器”，对应的算法为“基学习算法”。

异质集成：个体学习器称为“组件学习器”或直接称为“个体学习器”。

Boosting算法要求基学习器能对特定分布的数据进行学习，即每次都更新样 本分布权重。

重赋权法 : 对每个样本附加一个权重，这时涉及到样本属性与标签的计算，都需要乘上一个权值。

重采样法 : 对于一些无法接受带权样本的及学习算法，适合用“重采样法”进行处理。方法大致过程是，根据各个样本的权重，对训练数据进行重采样，初始时样本权重一样，每个样本被采样到的概率一致，每次从N个原始的训练样本中按照权重有放回采样N个样本作为训练集，然后计算训练集错误率，然后调整权重，重复采样，集成多个基学习器。

定义基学习器的集成为加权结合：![image-20201118201455664](集成学习.assets\image-20201118201455664.png)

指数损失函数：

![image-20201118201527192](集成学习.assets\image-20201118201527192.png)

基学习器之间的多样性是影响集成器泛化性能的重要因素，一般的思路是在学习过程中引入随机性，常见的做法主要是对数据样本、输入属性、输出表示、算法参数进行扰动。

结合策略指的是在训练好基学习器后，如何将这些基学习器的输出结合起来产生集成模型的最终输出，这里也有一些常用的结合策略。

 	1.平均法（主要用来解决回归问题）

​	 2.投票法（主要用来解决分类问题）



AdaBoost算法构造过程可以概括为：

 步骤一：初始化训练数据权重相等，训练第一个学习器;

 步骤二：AdaBoost反复学习基本分类器;

 步骤三：对m个学习器进行加权投票

## 算法原理

Adaboost算法的运行过程如下：训练数据中的每一个样本，并赋予其一个权重，这些权重构成了向量D。一开始，这些权重都初始化成相等值。首先在训练数据上训练出一个弱分类器并计算该分类器的错误率，然后在同一数据集上再次训练弱分类器。在分类器的第二次训练当中，将会重新调整每个样本的权重，其中第一次分对的样本的权重将会降低，而第一次分错的样本的权重将会提高。为了从所有的弱分类器中得到最终的分类结果，Adaboost为每个分类器都分配了一个权重值alpha,这些alpha值是基于每个弱分类器的错误率进行计算的。其中错误率的定义为：![image-20201118201816612](集成学习.assets\image-20201118201816612.png)

而权重值alpha的计算公式为：![image-20201118201835909](集成学习.assets\image-20201118201835909.png)

![image-20201118201849423](集成学习.assets\image-20201118201849423.png)

在上图中，左边是数据集，其中直方图的不同宽度表示每个样本上的不同权重。在经过一个分类器之后，加权的结果会通过三角形中的alpha值进行加权。每个三角形中输出的加权结果在椭圆形中求和，从而得到最终的输出结果.

计算出alpha值后，可以对权重向量D进行更新，以使得那些正确分类的样本的权重降低而错分样本的权重升高



**总结：**Adaboost算法的核心步骤:  一.计算基学习器的权重

​    														二,进行更新样本权重分布

权重向量D的计算方法如下：

①如果某个样本被正确分类，那么该样本的权重更改为：![image-20201118202133567](集成学习.assets\image-20201118202133567.png)

②如果某个样本被错误分类，那么该样本的权重更改为：![image-20201118202157390](集成学习.assets\image-20201118202157390.png)

在计算出D后，Adaboost又开始进入下一轮迭代。Adaboost算法会不断地重复训练和调整权重的过程，直到训练错误率为0或者弱分类器的数目达到用户的指定值为止。

## 算法示例

给定下面这张训练数据表所示的数据，假设弱分类器由x产生，其阈值v使该分类器在训练数据集上的分类误差率最低，试用Adaboost算法学习一个强分类器。

![image-20201118202251654](集成学习.assets\image-20201118202251654.png)

步骤一：初始化训练数据权重相等，训练第一个学习器：

D1=(w11,w12,...,w110,)

w1i=0.1, i=1,2,...,10

步骤二：AdaBoost反复学习基本分类器，在每一轮m=1,2,...,M顺次的执行下列操作：

（a）在权值分布为Dt的训练数据上，确定基分类器；

（b）计算该学习器上在训练数据上的错误率：![image-20201118202331717](集成学习.assets\image-20201118202331717.png)

（c）计算该学习器的投票权重：![image-20201118202350501](集成学习.assets\image-20201118202350501.png)

（d）根据投票权重对训练数据重新赋权![image-20201118202408040](集成学习.assets\image-20201118202408040.png)

将下一轮的学习器的注意力集中在错误数据中

重复a步到d步，执行m次。

步骤三：对m个学习器进行加权投票

![image-20201118202434943](集成学习.assets\image-20201118202434943.png)



**问题解答：**

步骤一：初始化训练数据权重相等，训练第一个学习器：

![image-20201118202547286](集成学习.assets\image-20201118202547286.png)

步骤二：AdaBoost反复学习基本分类器，在每一轮m=1,2,..., M顺次的执行下列操作：

当m=1的时候：（a）在权值分布为D1的训练数据上，阈值v取2.5时分类误差率最低，6,7,8被分错，故基本分类器为:![image-20201118202606601](集成学习.assets\image-20201118202606601.png)

（b）计算该学习器在训练数据中的错误率：![image-20201118202633577](集成学习.assets\image-20201118202633577.png)

（c）计算该学习器的投票权重：![image-20201118202640504](集成学习.assets\image-20201118202640504.png)

（d）根据投票权重，对训练数据重新赋权:![image-20201118202647767](集成学习.assets\image-20201118202647767.png)

根据下公式，计算各个权重值:

![image-20201118202722241](集成学习.assets\image-20201118202722241.png)

经过计算D2的值为：

![image-20201118202734719](集成学习.assets\image-20201118202734719.png)

计算过程：

![image-20201118202743111](集成学习.assets\image-20201118202743111.png)

![image-20201118202750247](集成学习.assets\image-20201118202750247.png)

分类器H1(x)训练数据集上有3个误分类点。

**当m=2的时候**

**（**a）在权值分布为2的训练数据上，阈值v取8.5时分类误差率最低，3,4,5被分错，故基本分类器为:

![image-20201118202816456](集成学习.assets\image-20201118202816456.png)

（b）计算该学习器在训练数据中的错误率：![image-20201118202922263](集成学习.assets\image-20201118202922263.png)

（c）计算该学习器的投票权重：![image-20201118202943711](集成学习.assets\image-20201118202943711.png)

（d）根据投票权重，对训练数据重新赋权:

经计算得，D3的值为：![image-20201118203003928](集成学习.assets\image-20201118203003928.png)

![image-20201118203011240](集成学习.assets\image-20201118203011240.png)

分类器H2 (x)在训练数据集上有3个误分类点。



**当m=3时的时候：**

（a）在权值分布为D3 的训练数据上，阈值v取5.5时分类误差率最低，故基本分类器为:

![image-20201118203057919](集成学习.assets\image-20201118203057919.png)

（b）计算该学习器在训练数据中的错误率：![image-20201118203123263](集成学习.assets\image-20201118203123263.png)

（c）计算该学习器的投票权重：![image-20201118203154250](集成学习.assets\image-20201118203154250.png)

（d）根据投票权重，对训练数据重新赋权:经计算得，D4 的值为：![image-20201118203211137](集成学习.assets\image-20201118203211137.png)

![image-20201118203217761](集成学习.assets\image-20201118203217761.png)

分类器H3 (x)在训练数据集上的误分类点个数为0。

步骤三：对m个学习器进行加权投票,获取最终分类器

![image-20201118203244234](集成学习.assets\image-20201118203244234.png)

## 算法代码

```
if __name__ == '__main__':
    #开始时间
    start = time.time()
    # 获取训练集
    print('start read transSet')
    trainDataList, trainLabelList = loadData('../Mnist/mnist_train.csv')
    # 获取测试集
    print('start read testSet')
    testDataList, testLabelList = loadData('../Mnist/mnist_test.csv')
    #创建提升树
    print('start init train')
    tree = createBosstingTree(trainDataList[:10000], trainLabelList[:10000], 40)
    #测试
    print('start to test')
    accuracy = model_test(testDataList[:1000], testLabelList[:1000], tree)
    print('the accuracy is:%d' % (accuracy * 100), '%')
    #结束时间
    end = time.time()
    print('time span:', end - start)

```

```
#coding=utf-8
#Author:dell

'''
数据集：Mnist
训练集数量：60000(实际使用：10000)
测试集数量：10000（实际使用：1000)
层数：40
------------------------------
运行结果：
    正确率：97%
    运行时长：65m
'''

import time
import numpy as np

def loadData(fileName):
    '''
    加载文件
    :param fileName:要加载的文件路径
    :return: 数据集和标签集
    '''
    #存放数据及标记
    dataArr = []; labelArr = []
    #读取文件
    fr = open(fileName)
    #遍历文件中的每一行
    for line in fr.readlines():
        #获取当前行，并按“，”切割成字段放入列表中
        #strip：去掉每行字符串首尾指定的字符（默认空格或换行符）
        #split：按照指定的字符将字符串切割成每个字段，返回列表形式
        curLine = line.strip().split(',')
        #将每行中除标记外的数据放入数据集中（curLine[0]为标记信息）
        #在放入的同时将原先字符串形式的数据转换为整型
        #此外将数据进行了二值化处理，大于128的转换成1，小于的转换成0，方便后续计算
        dataArr.append([int(int(num) > 128) for num in curLine[1:]])
        #将标记信息放入标记集中
        #放入的同时将标记转换为整型

        #转换成二分类任务
        #标签0设置为1，反之为-1
        if int(curLine[0]) == 0:
            labelArr.append(1)
        else:
            labelArr.append(-1)
    #返回数据集和标记
    return dataArr, labelArr

def calc_e_Gx(trainDataArr, trainLabelArr, n, div, rule, D):
    '''
    计算分类错误率
    :param trainDataArr:训练数据集数字
    :param trainLabelArr: 训练标签集数组
    :param n: 要操作的特征
    :param div:划分点
    :param rule:正反例标签
    :param D:权值分布D
    :return:预测结果， 分类误差率
    '''
    #初始化分类误差率为0
    e = 0
    #将训练数据矩阵中特征为n的那一列单独剥出来做成数组。因为其他元素我们并不需要，
    #直接对庞大的训练集进行操作的话会很慢
    x = trainDataArr[:, n]
    #同样将标签也转换成数组格式，x和y的转换只是单纯为了提高运行速度
    #测试过相对直接操作而言性能提升很大
    y = trainLabelArr
    predict = []

    #依据小于和大于的标签依据实际情况会不同，在这里直接进行设置
    if rule == 'LisOne':    L = 1; H = -1
    else:                   L = -1; H = 1

    #遍历所有样本的特征m
    for i in range(trainDataArr.shape[0]):
        if x[i] < div:
            #如果小于划分点，则预测为L
            #如果设置小于div为1，那么L就是1，
            #如果设置小于div为-1，L就是-1
            predict.append(L)
            #如果预测错误，分类错误率要加上该分错的样本的权值（8.1式）
            if y[i] != L: e += D[i]
        elif x[i] >= div:
            #与上面思想一样
            predict.append(H)
            if y[i] != H: e += D[i]
    #返回预测结果和分类错误率e
    #预测结果其实是为了后面做准备的，在算法8.1第四步式8.4中exp内部有个Gx，要用在那个地方
    #以此来更新新的D
    return np.array(predict), e

def createSigleBoostingTree(trainDataArr, trainLabelArr, D):
    '''
    创建单层提升树
    :param trainDataArr:训练数据集数组
    :param trainLabelArr: 训练标签集数组
    :param D: 算法8.1中的D
    :return: 创建的单层提升树
    '''

    #获得样本数目及特征数量
    m, n = np.shape(trainDataArr)
    #单层树的字典，用于存放当前层提升树的参数
    #也可以认为该字典代表了一层提升树
    sigleBoostTree = {}
    #初始化分类误差率，分类误差率在算法8.1步骤（2）（b）有提到
    #误差率最高也只能100%，因此初始化为1
    sigleBoostTree['e'] = 1

    #对每一个特征进行遍历，寻找用于划分的最合适的特征
    for i in range(n):
        #因为特征已经经过二值化，只能为0和1，因此分切分时分为-0.5， 0.5， 1.5三挡进行切割
        for div in [-0.5, 0.5, 1.5]:
            #在单个特征内对正反例进行划分时，有两种情况：
            #可能是小于某值的为1，大于某值得为-1，也可能小于某值得是-1，反之为1
            #因此在寻找最佳提升树的同时对于两种情况也需要遍历运行
            #LisOne：Low is one：小于某值得是1
            #HisOne：High is one：大于某值得是1
            for rule in ['LisOne', 'HisOne']:
                #按照第i个特征，以值div进行切割，进行当前设置得到的预测和分类错误率
                Gx, e = calc_e_Gx(trainDataArr, trainLabelArr, i, div, rule, D)
                #如果分类错误率e小于当前最小的e，那么将它作为最小的分类错误率保存
                if e < sigleBoostTree['e']:
                    sigleBoostTree['e'] = e
                    #同时也需要存储最优划分点、划分规则、预测结果、特征索引
                    #以便进行D更新和后续预测使用
                    sigleBoostTree['div'] = div
                    sigleBoostTree['rule'] = rule
                    sigleBoostTree['Gx'] = Gx
                    sigleBoostTree['feature'] = i
    #返回单层的提升树
    return sigleBoostTree

def createBosstingTree(trainDataList, trainLabelList, treeNum = 50):
    '''
    创建提升树
    创建算法依据“8.1.2 AdaBoost算法” 算法8.1
    :param trainDataList:训练数据集
    :param trainLabelList: 训练测试集
    :param treeNum: 树的层数
    :return: 提升树
    '''
    #将数据和标签转化为数组形式
    trainDataArr = np.array(trainDataList)
    trainLabelArr = np.array(trainLabelList)
    #没增加一层数后，当前最终预测结果列表
    finallpredict = [0] * len(trainLabelArr)
    #获得训练集数量以及特征个数
    m, n = np.shape(trainDataArr)

    #依据算法8.1步骤（1）初始化D为1/N
    D = [1 / m] * m
    #初始化提升树列表，每个位置为一层
    tree = []
    #循环创建提升树
    for i in range(treeNum):
        #得到当前层的提升树
        curTree = createSigleBoostingTree(trainDataArr, trainLabelArr, D)
        #根据式8.2计算当前层的alpha
        alpha = 1/2 * np.log((1 - curTree['e']) / curTree['e'])
        #获得当前层的预测结果，用于下一步更新D
        Gx = curTree['Gx']
        #依据式8.4更新D
        #考虑到该式每次只更新D中的一个w，要循环进行更新知道所有w更新结束会很复杂（其实
        #不是时间上的复杂，只是让人感觉每次单独更新一个很累），所以该式以向量相乘的形式，
        #一个式子将所有w全部更新完。
        #该式需要线性代数基础，如果不太熟练建议补充相关知识，当然了，单独更新w也一点问题
        #没有
        #np.multiply(trainLabelArr, Gx)：exp中的y*Gm(x)，结果是一个行向量，内部为yi*Gm(xi)
        #np.exp(-1 * alpha * np.multiply(trainLabelArr, Gx))：上面求出来的行向量内部全体
        #成员再乘以-αm，然后取对数，和书上式子一样，只不过书上式子内是一个数，这里是一个向量
        #D是一个行向量，取代了式中的wmi，然后D求和为Zm
        #书中的式子最后得出来一个数w，所有数w组合形成新的D
        #这里是直接得到一个向量，向量内元素是所有的w
        #本质上结果是相同的
        D = np.multiply(D, np.exp(-1 * alpha * np.multiply(trainLabelArr, Gx))) / sum(D)
        #在当前层参数中增加alpha参数，预测的时候需要用到
        curTree['alpha'] = alpha
        #将当前层添加到提升树索引中。
        tree.append(curTree)

        #-----以下代码用来辅助，可以去掉---------------
        #根据8.6式将结果加上当前层乘以α，得到目前的最终输出预测
        finallpredict += alpha * Gx
        #计算当前最终预测输出与实际标签之间的误差
        error = sum([1 for i in range(len(trainDataList)) if np.sign(finallpredict[i]) != trainLabelArr[i]])
        #计算当前最终误差率
        finallError = error / len(trainDataList)
        #如果误差为0，提前退出即可，因为没有必要再计算算了
        if finallError == 0:    return tree
        #打印一些信息
        print('iter:%d:%d, sigle error:%.4f, finall error:%.4f'%(i, treeNum, curTree['e'], finallError ))
    #返回整个提升树
    return tree

def predict(x, div, rule, feature):
    '''
    输出单独层预测结果
    :param x: 预测样本
    :param div: 划分点
    :param rule: 划分规则
    :param feature: 进行操作的特征
    :return:
    '''
    #依据划分规则定义小于及大于划分点的标签
    if rule == 'LisOne':    L = 1; H = -1
    else:                   L = -1; H = 1

    #判断预测结果
    if x[feature] < div: return L
    else:   return H

def model_test(testDataList, testLabelList, tree):
    '''
    测试
    :param testDataList:测试数据集
    :param testLabelList: 测试标签集
    :param tree: 提升树
    :return: 准确率
    '''
    #错误率计数值
    errorCnt = 0
    #遍历每一个测试样本
    for i in range(len(testDataList)):
        #预测结果值，初始为0
        result = 0
        #依据算法8.1式8.6
        #预测式子是一个求和式，对于每一层的结果都要进行一次累加
        #遍历每层的树
        for curTree in tree:
            #获取该层参数
            div = curTree['div']
            rule = curTree['rule']
            feature = curTree['feature']
            alpha = curTree['alpha']
            #将当前层结果加入预测中
            result += alpha * predict(testDataList[i], div, rule, feature)
        #预测结果取sign值，如果大于0 sign为1，反之为0
        if np.sign(result) != testLabelList[i]: errorCnt += 1
    #返回准确率
    return 1 - errorCnt / len(testDataList)

```































