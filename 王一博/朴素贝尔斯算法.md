#                      朴素贝尔斯算法

## 1.1学习目标

​           说明条件概率与联合概率

​           说明贝叶斯公式、以及特征独立的关系

​           记忆贝叶斯公式

​           知道拉普拉斯平滑系数

​           应用贝叶斯公式实现概率的计算

​          会使用朴素贝叶斯分析具体问题

## 1.2朴素贝叶斯算法简介

​        贝叶斯学派的思想可以概括为先验概率+数据=后验概率。也就是说我们在实际问题中需要得到的后验概率，可以通过先验概率和数据一起综合得到。

​       数据大家好理解，被频率学派攻击的是先验概率，一般来说先验概率就是我们对于数据所在领域的历史经验，但是这个经验常常难以量化或者模型化，于是贝叶斯学派大胆的假设先验分布的模型，比如正态分布，beta分布等。这个假设一般没有特定的依据，因此一直被频率学派认为很荒谬。虽然难以从严密的数学逻辑里推出贝叶斯学派的逻辑，但是在很多实际应用中，贝叶斯理论很好用，比如垃圾邮件分类，文章分类。

![图片1](C:\Users\67315\Desktop\新建文件夹\图片\图片1.png)





![图片2](C:\Users\67315\Desktop\新建文件夹\图片\图片2.png)





## 1.3数学知识

#### （1）概率定义

概率定义为一件事情发生的可能性。比如扔出一个硬币，结果头像朝上的概率。P(X) : 取值在[0, 1]。

#### （2）案例：判断女神对你的喜欢情况

在讲这两个概率之前我们通过一个例子，来计算一些结果：

![图片3](C:\Users\67315\Desktop\新建文件夹\图片\图片3.png)



问题如下：

##### 1.女神喜欢你的概率：

​     P(女神喜欢你)=4（图中女神喜欢有4个样本)/7（总共7个样本）

##### 2.在女神喜欢的条件下，职业是程序员的概率？

   P(程序员 | 女神喜欢)=2（4个喜欢中2个为程序员）/4 （女神喜欢总共有4个）

##### 3.在女神喜欢的条件下，职业是程序员、体重超重的概率？

P(程序员、体重超重|女神喜欢)=1(只有样本4)/4（女神喜欢有4个样本）

###### 联合概率：包含多个条件，且所有条件同时成立的概率

###### 条件概率：就是事件A在另外一个事件B已经发生条件下的发生概率记作：P(A|B

###### 相互独立：如果P(A, B) = P(A)P(B)，则称事件A与事件B相互独立。

## 1.4贝叶斯公式

### 公式介绍

![图片4](C:\Users\67315\Desktop\新建文件夹\图片\图片4.png)



## 1.5案例计算

![图片5](C:\Users\67315\Desktop\新建文件夹\图片\图片5.png)



现在给我们的问题是，如果一对男女朋友，男生想女生求婚，男生的四个特点分别是不帅，性格不好，身高矮，不上进，请你判断一下女生是嫁还是不嫁？

​    这是一个典型的分类问题，转为数学问题就是比较p(嫁|(不帅、性格不好、身高矮、不上进))与p(不嫁|(不帅、性格不好、身高矮、不上进))的概率，谁的概率大，我就能给出嫁或者不嫁的答案！

#### 我们联系到朴素贝叶斯公式：

![图片6](C:\Users\67315\Desktop\新建文件夹\图片\图片6.png)





​    我们需要求p(嫁|(不帅、性格不好、身高矮、不上进),这是我们不知道的，但是通过朴素贝叶斯公式可以转化为好求的三个量，p(不帅、性格不好、身高矮、不上进|嫁)、p（不帅、性格不好、身高矮、不上进)、p(嫁)（至于为什么能求，后面会讲，那么就太好了，将待求的量转化为其它可求的值，这就相当于解决了我们的问题！）

​	那么这三个量是如何求得？

是根据已知训练数据统计得来，下面详细给出该例子的求解过程。

那么我只要求得p(不帅、性格不好、身高矮、不上进|嫁)、p（不帅、性格不好、身高矮、不上进)、p(嫁)即可，好的，下面我分别求出这几个概率，最后一比，就得到最终结果。

p(不帅、性格不好、身高矮、不上进|嫁) = p(不帅|嫁)\*p(性格不好|嫁)\*p(身高矮|嫁)\*p(不上进|嫁)，那么我就要分别统计后面几个概率，也就得到了左边的概率！朴素贝叶斯算法是假设各个特征之间相互独立，那么这个等式就成立了！

#### 但是为什么需要假设特征之间相互独立呢？

1、我们这么想，假如没有这个假设，那么我们对右边这些概率的估计其实是不可做的，这么说，我们这个例子有4个特征，其中帅包括{帅，不帅}，性格包括{不好，好，爆好}，身高包括{高，矮，中}，上进包括{不上进，上进}，那么四个特征的联合概率分布总共是4维空间，总个数为2*3*3*2=36个。

36个，计算机扫描统计还可以，但是现实生活中，往往有非常多的特征，每一个特征的取值也是非常之多，那么通过统计来估计后面概率的值，变得几乎不可做，这也是为什么需要假设特征之间独立的原因。

2.假如我们没有假设特征之间相互独立，那么我们统计的时候，就需要在整个特征空间中去找，比如统计p(不帅、性格不好、身高矮、不上进|嫁),我们就需要在嫁的条件下，去找四种特征全满足分别是不帅，性格不好，身高矮，不上进的人的个数，这样的话，由于数据的稀疏性，很容易统计到0的情况。 这样是不合适的。

***\*根据上面俩个原因，朴素贝叶斯法对条件概率分布做了条件独立性的假设，由于这是一个较强的假设，朴素贝叶斯也由此得名！这一假设使得朴素贝叶斯法变得简单，但有时会牺牲一定的分类准确率。\****

首先我们整理训练数据中，嫁的样本数如下：

![图片7](C:\Users\67315\Desktop\新建文件夹\图片\图片7.png)



##### 则 p(嫁) = 6/12（总样本数） = 1/2

![图片8](C:\Users\67315\Desktop\新建文件夹\图片\图片8.png)



##### p(不帅|嫁) = 3/6=1/2

![图片9](C:\Users\67315\Desktop\新建文件夹\图片\图片9.png)



##### 则p(性格不好|嫁)= 1/6

![图片10](C:\Users\67315\Desktop\新建文件夹\图片\图片10.png)



##### 则p(矮|嫁) = 1/6

![图片11](C:\Users\67315\Desktop\新建文件夹\图片\图片11.png)



##### 则p(不上进|嫁) = 1/6

#### 下面开始求分母，p(不帅)，p（性格不好），p（矮），p（不上进）



##### 不帅统计如上红色所示，占4个，那么p（不帅） = 4/12 = 1/3

![图片14](C:\Users\67315\Desktop\新建文件夹\图片\图片14.png)

同理：

##### 性格不好统计如上红色所示，占4个，那么p（性格不好） = 4/12 = 1/3

##### 身高矮统计如上红色所示，占7个，那么p（身高矮） = 7/12

##### 上进统计如上红色所示，占4个，那么p（不上进） = 4/12 = 1/3





##### 则p（矮|不嫁） = 6/6 

##### p(不嫁)=6/12 = 1/2

##### p（不帅|不嫁） = 1/6

##### 则p（性格不好|不嫁） =3/6 = 1/2

##### 则p（不上进|不嫁） = 3/6 = 1/2



![图片13](C:\Users\67315\Desktop\新建文件夹\图片\图片13.png)



将上边算的带入公式即可：

很显然p(不嫁|不帅、性格不好、身材矮、不上进)>p(嫁|不帅、性格不好、身材矮、不上进)

所以判断不嫁

## 1.6案例代码

![1](C:\Users\67315\Desktop\新建文件夹\图片\1.jpg)



#### 1 步骤分析

- 1）获取数据

- 2）数据基本处理

  - 2.1） 取出内容列，对数据进行分析
  - 2.2） 判定评判标准
  - 2.3） 选择停用词
  - 2.4） 把内容处理，转化成标准格式
  - 2.5） 统计词的个数
  - 2.6）准备训练集和测试集

- 3）模型训练

- 4）模型评估

  #### 2 代码实现

  ```python
  import pandas as pd
  import numpy as np
  import jieba
  import matplotlib.pyplot as plt
  from sklearn.feature_extraction.text import CountVectorizer
  from sklearn.naive_bayes import MultinomialNB
  ```

1）获取数据

```python
# 加载数据
data = pd.read_csv("./data/书籍评价.csv", encoding="gbk")
data
```

2）数据基本处理

```python
# 2.1） 取出内容列，对数据进行分析
content = data["内容"]
content.head()

# 2.2） 判定评判标准 -- 1好评;0差评
data.loc[data.loc[:, '评价'] == "好评", "评论标号"] = 1  # 把好评修改为1
data.loc[data.loc[:, '评价'] == '差评', '评论标号'] = 0

# data.head()
good_or_bad = data['评价'].values  # 获取数据
print(good_or_bad)
# ['好评' '好评' '好评' '好评' '差评' '差评' '差评' '差评' '差评' '好评' '差评' '差评' '差评']

# 2.3） 选择停用词
# 加载停用词
stopwords=[]
with open('./data/stopwords.txt','r',encoding='utf-8') as f:
    lines=f.readlines()
    print(lines)
    for tmp in lines:
        line=tmp.strip()
        print(line)
        stopwords.append(line)
# stopwords  # 查看新产生列表

#对停用词表进行去重
stopwords=list(set(stopwords))#去重  列表形式
print(stopwords)

# 2.4） 把“内容”处理，转化成标准格式
comment_list = []
for tmp in content:
    print(tmp)
    # 对文本数据进行切割
    # cut_all 参数默认为 False,所有使用 cut 方法时默认为精确模式
    seg_list = jieba.cut(tmp, cut_all=False)
    print(seg_list)  # <generator object Tokenizer.cut at 0x0000000007CF7DB0>
    seg_str = ','.join(seg_list)  # 拼接字符串
    print(seg_str)
    comment_list.append(seg_str)  # 目的是转化成列表形式
# print(comment_list)  # 查看comment_list列表。

# 2.5） 统计词的个数
# 进行统计词个数
# 实例化对象
# CountVectorizer 类会将文本中的词语转换为词频矩阵
con = CountVectorizer(stop_words=stopwords)
# 进行词数统计
X = con.fit_transform(comment_list)  # 它通过 fit_transform 函数计算各个词语出现的次数
name = con.get_feature_names()  # 通过 get_feature_names()可获取词袋中所有文本的关键字
print(X.toarray())  # 通过 toarray()可看到词频矩阵的结果
print(name)

# 2.6）准备训练集和测试集
# 准备训练集   这里将文本前10行当做训练集  后3行当做测试集
x_train = X.toarray()[:10, :]
y_train = good_or_bad[:10]
# 准备测试集
x_text = X.toarray()[10:, :]
y_text = good_or_bad[10:]
```

3）模型训练

```python
# 构建贝叶斯算法分类器
mb = MultinomialNB(alpha=1)  # alpha 为可选项，默认 1.0，添加拉普拉修/Lidstone 平滑参数
# 训练数据
mb.fit(x_train, y_train)
# 预测数据
y_predict = mb.predict(x_text)
#预测值与真实值展示
print('预测值：',y_predict)
print('真实值：',y_text)
```

4）模型评估

```python
mb.score(x_text, y_text)
```

## 1.7总结

#### 优点：

（1） 算法逻辑简单,易于实现（算法思路很简单，只要使用贝叶斯公式转化数学即可！）
（2）分类过程中时空开销小（假设特征相互独立，只会涉及到二维存储）

#### 缺点：

理论上，朴素贝叶斯模型与其他分类方法相比具有最小的误差率。但是实际上并非总是如此，这是因为朴素贝叶斯模型假设属性之间相互独立，这个假设在实际应用中往往是不成立的，在属性个数比较多或者属性之间相关性较大时，分类效果不好。
而在属性相关性较小时，朴素贝叶斯性能最为良好。对于这一点，有半朴素贝叶斯之类的算法通过考虑部分关联性适度改进。