---
typora-copy-images-to: ./
---

# 决策树算法C4.5

## 决策树

招聘机器学习算法工程师

![](C:\Users\皮朋朋\Desktop\皮朋朋作业\图片1.png)

1.非参数学习算法

2.可以解决分类问题

3.天然可以解决多分类问题

4.也可以解决回归问题

5.非常好的可解释性

![](C:\Users\皮朋朋\Desktop\皮朋朋作业\图片二.png)

问题：

每个节点在哪个维度做划分？

某个维度在哪个值上做划分？

## C4.5算法简介与概括

•1993年由Quinlan提出的C4.5算法（对ID3算法的改进）

•C4.5比ID3的改进：

1.用信息增益率来选择属性，克服了用信息增益选择属性时偏向选择取值多的属性的不足；

2.在树构造过程中进行剪枝；

3.能够完成对连续属性的离散化处理；

4.能够对不完整数据进行处理；

•C4.5算法优点：产生的分类规则易于理解，准确率较高。

•C4.5算法缺点：在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效。

## 信息熵

•1948年，香农提出了“信息熵”的概念，解决了对系统信息的量化度量问题。

•香农认为信息的准确信息量可以用下面的信息熵公式计算：

![](C:\Users\皮朋朋\Desktop\皮朋朋作业\图片三.png)

•其中，S表示样本集；

•K表示样本集合中类别个数（只含正负样本，则K=2);

•pi表示第i个类的概率；（ pi可由类别i中含有的样本个数除以总样本数得到）

例：

{1/3，1/3， 1/3}                                									{1/10， 2/10，7/10}

![](C:\Users\皮朋朋\Desktop\皮朋朋作业\图片四.png)

•一个系统越是有序，信息熵就越低；反之，一个系统越乱，信息熵就越高。所以，信息熵也可以说是系统有序化程度的一个衡量。

## 信息增益 

​    假定离散属性F有V个可能的取值，若使用F对样本集S进行划分，则产生V个分支节点，考虑到不同的分支节点所包含的样本数不同，给分支节点赋予权重|SV|/|S|，即样本数越多的分支节点的影响越大，则信息增益：

 ![](C:\Users\皮朋朋\Desktop\皮朋朋作业\图片五.png)

   一般而言，信息增益越大，则意味着使用属性F来进行划分所获得的“纯度提升”越大。著名的ID3决策树学习算法就是以信息增益为准则来进行划分属性。



**缺点**：实际上，信息增益准则对可取值数目较多的属性有所偏好。

## 信息增益率

•与ID3不同，C4.5采用基于信息增益率（information Gain Ratio)的方法选择测试属性，信息增益率等于信息增益对分割信息量的比值。

•GainRatio(S,F)=Gain(S,F)/SplitInformation(S,F) 

•设定样本集S按离散属性F的V个不同的取值划分为，共V个子集

•定义分割信息量Split(S,F)：

![](C:\Users\皮朋朋\Desktop\皮朋朋作业\图片六.png)

•那么信息增益率为：

 GainRatio(S,F)=Gain(S,F)/Split(S,F)

## 连续型属性

•离散化处理：将连续型的属性变量进行离散化处理，形成决策树的训练集

•把需要处理的样本（对应根节点）或样本子集（对应子树）按照连续变量的大小从小到大进行排序

•假设该属性对应的不同属性值一共有N个，那么总共N-1个有可能的候选分割阈值点，每个候选的分割阈值点的值为上述排序后的属性值中两两前后连续元素的中点

•用信息增益率选择最佳划分

## 属性值缺失

•  缺失值：在某些情况下，可供使用的数据可能缺少某些属性的值。例如(X,Y)是样本集S的一个训练实例，X=(F1-v,F2-v,…,Fn-v)。但是其属性Fi的值Fi-v未知。

•处理策略：

•处理缺少属性值的一种策略是赋给它结点t所对应的训练实例中该属性的最常见值

•另一种更复杂的策略是为Fi的每一个可能值赋予一个概率。例如，给定一个布尔属性Fi,如果t包含6个已知Fi-v=1和4个Fi-v=0的实例，那么Fi-v=1的概率是0.6，而Fi-v=0的概率是0.4。于是，实例x的60%被分配到Fi-v=1的分支，40%被分配到另一个分支。这些片段样例的目的是为了计算信息增益，另外，如果有第二个缺少值的属性必须被测试，这些样例可以在后继的树分支中被进一步细分。

•简单处理策略就是丢弃这些样本。

## 过拟合问题

过拟合：有监督的算法需要考虑泛化能力，在有限样本的条件下，决策树超过一定规模后，训练错误率会减小，但测试错误率会增加。

为了避免过拟合，采用剪枝技术

剪枝：控制决策树规模的方法叫做剪枝，一种是预剪枝，一种是后剪枝。所谓预剪枝就是控制决策树的生长；后剪枝是指对完全生成的决策树进行修剪。

### 预剪枝（pre-pruning)：

1.数据划分法：数据划分成训练样本和测试样本，使用训练样本进行训练，使用测试样本进行树生长检测。

2.阈值法：当某节点的信息增益小于某阈值时，停止树生长。

3.信息增益的统计显著性分析：从已有节点获得的所有信息增益统计其分布，如果继续生长得到的信息增益与该分布相比不显著，则停止树的生长。

优点：简单直接。

缺点：对于不回朔的贪婪算法，缺乏后效性考虑，可能导致树提前停止。

### 后剪枝(post-pruning）:

1.减少分类错误修建法：使用独立的剪枝集估计剪枝前后的分类错误率，基于此进行剪枝。

2.最小代价与复杂性折中的剪枝：对剪枝后的树综合评价错误率复杂性，决定是否剪枝。

3.最小描述长度准则：最简单的数就是最好的树。对决策树进行编码，通过剪枝得到编码最小的树

4.规则后剪枝：将训练完的决策树转换成规则，通过删除不会降低估计精度的前提下修剪每一条规则。

优点：实际应用有效

缺点：数据量大时，计算代价比较大

常见的后剪枝方法有：Reduced-Error(REP,错误率降低剪枝)，Pessimistic Error Pruning(PEP,悲观剪枝) 和Cost-Complexity Pruning(CCP,代价复杂度)

​    决策树方法中的CART,ID3,C4.5算法主要采用后剪枝技术。后剪枝操作是一个边修剪边检验的过程，一般规则标准是：在决策树的不断剪枝操作过程中，将原样本集合或新数据集合作为测试数据，检测决策树对测试数据的预测精度，并计算出相应的错误率，如果剪掉某个子树后的决策树对测试数据的预测精度或者其他测度不降低，那么剪掉该子树。

C4.5采用Pessimistic Error Pruning(PEP，悲观剪枝)，它使用训练集生成决策树又用它来进行剪枝，不需要独立的剪枝集。

## 算法流程：

I.选择节点分裂属性

II.建立新的节点，划分数据集

III.判断节点是否到生长停止条件，如果是，终止生长，如果不是转到Ι

## C4.5算法步骤示意

C4.5(DataSet, featureList):

创建根节点R

如果当前DataSet中的数据都属于同一类，则标记R的类别为该类

如果当前featureList集合为空，则标记R的类别为当前DataSet中样本最多的类别

递归情况：

​	从featureList中选择属性F(选择GainRatio(DataSet,F)最大的属性，选择属性参见上面的离散化过程）

​	根据F的每一个值v，将DataSet划分为不同的子集DS，对于每一个DS：

​		创建节点C

​		如果DS为空，节点C标记为DataSet中样本最多的类别

​		如果DS不为空，节点C=C4.5(DS，featureList-F)

​		将节点C添加为R的子节点

# 演示样例

![](C:\Users\皮朋朋\Desktop\皮朋朋作业\图片七.png)

# 算法代码

## 为什么要改进成C4.5算法？

　　C4.5算法是在ID3算法上的一种改进，它与ID3算法最大的区别就是特征选择上有所不同，一个是基于信息增益比，一个是基于信息增益。

　　之所以这样做是因为信息增益倾向于选择取值比较多的特征(特征越多，条件熵(特征划分后的类别变量的熵)越小，信息增益就越大)；因此在信息增益下面加一个分母，该分母是当前所选特征的熵，注意：这里而不是类别变量的熵了。

　　这样就构成了新的特征选择准则，叫做信息增益比。为什么加了这样一个分母就会消除ID3算法倾向于选择取值较多的特征呢？

　　因为特征取值越多，该特征的熵就越大，分母也就越大，所以信息增益比就会减小，而不是像信息增益那样增大了，一定程度消除了算法对特征取值范围的影响。

    实现

　　在算法实现上，C4.5算法只是修改了信息增益计算的函数calcShannonEntOfFeature和最优特征选择函数chooseBestFeatureToSplit。

　　calcShannonEntOfFeature在ID3的calcShannonEnt函数上加了个参数feat，ID3中该函数只用计算类别变量的熵，而calcShannonEntOfFeature可以计算指定特征或者类别变量的熵。

　　chooseBestFeatureToSplit函数在计算好信息增益后，同时计算了当前特征的熵IV，然后相除得到信息增益比，以最大信息增益比作为最优特征。

　　在划分数据的时候，有可能出现特征取同一个值，那么该特征的熵为0，同时信息增益也为0(类别变量划分前后一样，因为特征只有一个取值)，0/0没有意义，可以跳过该特征。
#coding=utf-8
import operator
from math import log
import time
import os, sys
import string

def createDataSet(trainDataFile):
    print trainDataFile
    dataSet = []
    try:
        fin = open(trainDataFile)
        for line in fin:
            line = line.strip()
            cols = line.split('\t')
            row = [cols[1], cols[2], cols[3], cols[4], cols[5], cols[6], cols[7], cols[8], cols[9], cols[10], cols[0]]
            dataSet.append(row)
            #print row
    except:
        print 'Usage xxx.py trainDataFilePath'
        sys.exit()
        labels = ['cip1', 'cip2', 'cip3', 'cip4', 'sip1', 'sip2', 'sip3', 'sip4', 'sport', 'domain']
    print 'dataSetlen', len(dataSet)
        return dataSet, labels

#calc shannon entropy of label or feature
def calcShannonEntOfFeature(dataSet, feat):
    numEntries = len(dataSet)
    labelCounts = {}
    for feaVec in dataSet:
        currentLabel = feaVec[feat]
        if currentLabel not in labelCounts:
            labelCounts[currentLabel] = 0
        labelCounts[currentLabel] += 1
    shannonEnt = 0.0
    for key in labelCounts:
        prob = float(labelCounts[key])/numEntries
        shannonEnt -= prob * log(prob, 2)
    return shannonEnt

def splitDataSet(dataSet, axis, value):
    retDataSet = []
    for featVec in dataSet:
        if featVec[axis] == value:
            reducedFeatVec = featVec[:axis]
            reducedFeatVec.extend(featVec[axis+1:])
            retDataSet.append(reducedFeatVec)
    return retDataSet
    
def chooseBestFeatureToSplit(dataSet):
    numFeatures = len(dataSet[0]) - 1    #last col is label
    baseEntropy = calcShannonEntOfFeature(dataSet, -1)
    bestInfoGainRate = 0.0
    bestFeature = -1
    for i in range(numFeatures):
        featList = [example[i] for example in dataSet]
        uniqueVals = set(featList)
        newEntropy = 0.0
        for value in uniqueVals:
            subDataSet = splitDataSet(dataSet, i, value)
            prob = len(subDataSet) / float(len(dataSet))
            newEntropy += prob *calcShannonEntOfFeature(subDataSet, -1)    #calc conditional entropy
        infoGain = baseEntropy - newEntropy
    　　 iv = calcShannonEntOfFeature(dataSet, i)
        if(iv == 0):    #value of the feature is all same,infoGain and iv all equal 0, skip the feature
        continue
    　　 infoGainRate = infoGain / iv
        if infoGainRate > bestInfoGainRate:
            bestInfoGainRate = infoGainRate
            bestFeature = i
    return bestFeature
            
#feature is exhaustive, reture what you want label
def majorityCnt(classList):
    classCount = {}
    for vote in classList:
        if vote not in classCount.keys():
            classCount[vote] = 0
        classCount[vote] += 1
    return max(classCount)         
    
def createTree(dataSet, labels):
    classList = [example[-1] for example in dataSet]
    if classList.count(classList[0]) ==len(classList):    #all data is the same label
        return classList[0]
    if len(dataSet[0]) == 1:    #all feature is exhaustive
        return majorityCnt(classList)
    bestFeat = chooseBestFeatureToSplit(dataSet)
    bestFeatLabel = labels[bestFeat]
    if(bestFeat == -1):        #特征一样，但类别不一样，即类别与特征不相关，随机选第一个类别做分类结果
    return classList[0] 
    myTree = {bestFeatLabel:{}}
    del(labels[bestFeat])
    featValues = [example[bestFeat] for example in dataSet]
    uniqueVals = set(featValues)
    for value in uniqueVals:
        subLabels = labels[:]
        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value),subLabels)
    return myTree
    
def main():
    if(len(sys.argv) < 3):
    print 'Usage xxx.py trainSet outputTreeFile'
    sys.exit()
    data,label = createDataSet(sys.argv[1])
    t1 = time.clock()
    myTree = createTree(data,label)
    t2 = time.clock()
    fout = open(sys.argv[2], 'w')
    fout.write(str(myTree))
    fout.close()
    print 'execute for ',t2-t1
if __name__=='__main__':
    main()