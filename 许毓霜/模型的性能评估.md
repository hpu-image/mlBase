# 模型的性能评估

## 概述

在机器学习领域中，对模型的评估非常重要，只有选择和问题相匹配的评估方法，才能快速发现算法模型或者训练过程的问题，迭代地对模型进行优化。模型评估主要分为离线评估和在线评估两个阶段。并且针对分类、回归、排序、序列预测等不同类型的机器学习问题，评估指标的选择也有所不同。在建模过程中，由于偏差过大导致的模型欠拟合以及方差过大导致的过拟合的存在，为了解决这两个问题，我们需要一整套方法及评价指标。其中评估方法用于评估模型的泛化能力，而性能指标则用于评价单个模型性能的高低。对学习器的泛化能力进行评估，不仅需要有效可行的实验估计方法，还需要有衡量模型泛化能力的评价标准，这就是性能度量。性能度量反应了任务需求，在对比不同模型的能力时，使用不同的性能度量往往会导致不同的评价结果；这意味着模型的“好坏”是相对的，什么样的模型是好的，不仅取决于算法与数据，还决定于任务需求。

## 基本概念

### 混淆矩阵(Confusion matrix）

对于二分类问题，可将样本根据其真实类别与学习器预测类别的组合划分为真正例（True Positive）、假正例（False Positive）、真反例（True Negative）、假反例（False Negative）四种情形，则显然有TP+FP+TN+FN=样本总数。分类结果的“混淆矩阵”（Confusion matrix）如下图所示：

![img](https://upload-images.jianshu.io/upload_images/11932934-a26fae68ee5721e4.png?imageMogr2/auto-orient/strip|imageView2/2/format/webp)

真正例（True Positive，TP）：预测的结果是Positive(+)，真实的结果是Positive(+)。

真反例（True Negative，TN）：预测的结果是Negative(-)，真实的结果是Negative(-)。

假正例（False Positive，FP）：预测的结果是Positive(+)，真实的结果是Negative(-)。

假反例（False Negative，FN）：预测的结果是Negative(-)，真实的结果是Positive(+)。

#### 1.分类准确率（Accuracy）

预测正确样本数与总样本数之比。

​                               ![image-20201118102052984](C:\Users\86150\AppData\Roaming\Typora\typora-user-images\image-20201118102052984.png)                                          

#### 2.精准率（Precision）

又称为查准率，表示预测结果为正的样本中（混淆矩阵中Positive(+)列）有多少预测准确。

![image-20201118102341487](C:\Users\86150\AppData\Roaming\Typora\typora-user-images\image-20201118102341487.png)

#### 3.召回率（Recall）

又称为查全率，表示真实结果为正的样本中（混淆矩阵中Positive(+）行）有多少被预测出来。

![image-20201118102544502](C:\Users\86150\AppData\Roaming\Typora\typora-user-images\image-20201118102544502.png)

#### 4.P-R曲线（Trade-off between precision and recall）

P-R曲线就是反应精准率precision与召回率recall曲线，以recall作为横轴，precision作为纵坐标轴。

很多模型在样本预测的时候并不是单纯的得出一个正、负这样的分类值，而是得到一个介于0和1之间的概率值，表示预测为正的概率为多大，然后通过这个数和一个阈值比较（通常为0.5），比这个阈值大的预测为正例，比这个阈值小的预测为反例。

当我们调整这个阈值的时候，我们预测的结果就会改变，所以，绘制P-R曲线的时候是以这个阈值为自变量，阈值改变的时候，查准率和查全率就会改变，将不同的阈值对应的precision和recall的值绘制到图形上面就形成了P-R曲线图。

刚开始，阈值很高，这样预测出来的样本当中正例样本很少，但是能保证这些预测为正的样本的确是真正的样本，也就是P值很高。随着阈值的下降，到了极限阈值为0的时候，所有样本都预测为正样本，此时R值最大，即所有为正的样本都被预测出来了。

这里有关于在不同阈值下，查准率与查全率的反应图：

![image-20201117194555649](C:\Users\86150\AppData\Roaming\Typora\typora-user-images\image-20201117194555649.png)

当阈值很高时，被预测出来的样本中正例样本很少，但是高阈值能保证这些预测值为正的样本是真实值为正的样本，故查准率很高。相反，真实值为正的样本被找出来的很少，召回率很低。所以阈值很高时，查准率很高，查全率很低。

随着阈值降低，查准率逐渐降低，查全率逐渐升高。

绘制出来的P-R曲线的一般是这样的：

![img](https://upload-images.jianshu.io/upload_images/11932934-75c1a6883a771519.png?imageMogr2/auto-orient/strip|imageView2/2/format/webp)

关于P-R图形有这么一个直观的认识：若一个模型的曲线把另外一个模型的曲线包住，那么这个模型的性能要优于另一个模型。

 提高分类阈值，则假证例的个数会显著减少，真正例的个数会减少或者不变，所以，此时precision会增加，recall会减少。

对于精准率与召回率，在不同的场景中，我们对于精准率与召回率这两个指标的重视程度是不同的：

当查准率更重要时：假设我们想去预测y = 1，（一个人患有癌症的情况），这时需要提高阈值到0.8，

预测为1时，阈值大于0.8

预测为0时，阈值小于0.8，这种提高阈值的做法提高了查准率，同时降低了查全率。

当查全率更重要时：假设在具体的场景中，不想要丢失太多y = 1的情况，此时我们需要降低阈值到0.3

预测为1时，阈值大于0.3

预测为0时，阈值小于0.3，这种降低阈值的做法提高了查全率，同时降低了查准率。

#### 5.F1 Score

当在一定的场景中，我们希望同时关注精准率与召回率，也就是说希望获得精准率与召回率的平衡，此时就需要一个新的指标为F1 Score，F1 Score是precision和recall的调和平均值。

F1 Score是precision和recall的调和平均值：![image-20201118194426544](C:\Users\86150\AppData\Roaming\Typora\typora-user-images\image-20201118194426544.png)

![image-20201118194326190](C:\Users\86150\AppData\Roaming\Typora\typora-user-images\image-20201118194326190.png)

关于F1 Score具体实例如下：

![img](https://upload-images.jianshu.io/upload_images/11932934-e8034030aa383709.png?imageMogr2/auto-orient/strip|imageView2/2/format/webp)



#### 6.ROC曲线

比如在逻辑回归里，设置一个阈值，大于这个阈值的为正类，小于这个阈值的为负类。假如减小阈值，样本中被识别为正类的样本增多，从而提高正类的识别率，同时使样本中的负类被错误的识别正类，因此引入ROC曲线，其可以用于评价一个分类器的好坏。

在混淆矩阵当中还可以定义两个变量，也是可以ROC曲线中的两个变量：

真正例率（True Positive Rate，TPR）：模型正确预测的正样本率（与召回率一样）。TPR=TP/(TP+FN）

假正例率（False Positive Rate，FPR）：模型错误预测的负样本率。FPR=FP/(FP+TN)

这里有关于在不同阈值下，FPR与TPR的变换过程图：

![image-20201117170004061](C:\Users\86150\AppData\Roaming\Typora\typora-user-images\image-20201117170004061.png)

在score分布了很多样本，当取不同的阈值来进行分类的时候，相应的TPR和FPR是怎样变化的

当阈值很高时，被预测出来的样本中正例样本很少，但是高阈值能保证这些预测值为正的样本是真实值为正的样本，真实值为正的样本很少被找出来，TPR（也可以说是召回率）很低，同时，由于高阈值，预测值为正，真实值为负的FP变少，FPR降低

随着阈值降低，FPR与TPR升高。

FPR与TPR呈现相一致的趋势变化，FPR越高，TPR越高，反之亦然。

当提高TPR值时，阈值降低，同时FPR降低，反之亦然。

ROC曲线就是反映了TPR与FPR之间的关系，如下图所示：

![img](https://upload-images.jianshu.io/upload_images/11932934-ae4cd3c12ac6e2f9.png?imageMogr2/auto-orient/strip|imageView2/2/format/webp)

判定的方法：曲线越靠近左上角，分类器的性能就越好。

#### 7.AUC（Area Under Curve）

随机挑选一个正样本以及一个负样本，分类器判定正样本的值高于负样本的概率就是AUC值

判定方法：AUC值越大的分类器，性能越好。

#### ![img](https://upload-images.jianshu.io/upload_images/11932934-4e87373fd38f829f.png?imageMogr2/auto-orient/strip|imageView2/2/format/webp)

## 模型性能评估的python实现

混淆矩阵

```python
import numpy as np
from sklearn import datasets
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score


# 1.获取数据集
digits = datasets.load_digits()
x = digits.data
y = digits.target.copy()

# 把数字数据集10个类别的问题变成了一个二分类问题
y[digits.target == 9] = 1
y[digits.target != 9] = 0

# 2.划分数据集
x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=666)

# 逻辑回归
log_reg = LogisticRegression()
log_reg.fit(x_train, y_train)
scores = log_reg.score(x_test, y_test)
decision_scores = log_reg.decision_function(x_test)
print("逻辑回归算法在测试集上的表现：\n", scores)

# 模型评估
y_log_reg_predict = log_reg.predict(x_test)

# 在scikit-learn中的混淆矩阵，精准率和召回率(调用相应的库函数)
matrix = confusion_matrix(y_test, y_log_reg_predict)
print("混淆矩阵：\n", matrix)
precision = precision_score(y_test, y_log_reg_predict)
print("模型评估结果精准率：\n", precision)
recall = recall_score(y_test, y_log_reg_predict)
print("模型评估结果的召回率：\n", recall)
f1 = f1_score(y_test, y_log_reg_predict)
print("f1_score的值：\n", f1)

# # scikit-learn中的Precision-Recall曲线
precisions, recalls, thresholds = precision_recall_curve(y_test, decision_scores)

# 不同域值下的，precision与recall变化情况
plt.plot(thresholds, precisions[:-1])
plt.plot(thresholds, recalls[:-1])
plt.show()

# precision与recall变化情况
plt.plot(precisions, recalls)
plt.show()

# scikit_learning中的ROC
fprs, tprs, thresholds = roc_curve(y_test, decision_scores)
plt.plot(fprs, tprs)
plt.show()

# roc_auc_score表示曲线下面积的指标
auc = roc_auc_score(y_test, decision_scores)
print("auc的值：\n", auc)
```

运行结果：

混淆矩阵：
 [[403   2]
 [  9  36]]
模型评估结果精准率：
 0.9473684210526315
模型评估结果的召回率：
 0.8
f1_score的值：
 0.8674698795180723
auc的值：
 0.9823319615912208

P-R曲线：

![image-20201117213913592](C:\Users\86150\AppData\Roaming\Typora\typora-user-images\image-20201117213913592.png)



<img src="C:\Users\86150\AppData\Roaming\Typora\typora-user-images\image-20201119220640297.png" alt="image-20201119220640297" style="zoom: 67%;" />



ROC曲线：

![image-20201117213956471](C:\Users\86150\AppData\Roaming\Typora\typora-user-images\image-20201117213956471.png)



参考：

周志华《机器学习》2.3 性能度量

[模型性能评估](https://www.jianshu.com/p/74ded053abef?utm_campaign=maleskine&utm_content=note&utm_medium=seo_notes&utm_source=recommendation)

[模型的性能评估（一）理论篇](https://www.cnblogs.com/jiaxin359/p/8622385.html)